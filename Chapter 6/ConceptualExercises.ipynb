{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Conceptual Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Best subset, forward stepwise and backward stepwise selection. For each approach, we get *p+1* models, containing 0, 1, 2, ..., *p* predictors.\n",
    "\n",
    "a) The smallest training RSS (residual sum of squares) will be obtained from the **best subset** containing *k* predictors. This is because in the best subset approach we fit the model in all the $\\binom{n}{k}$ subsets. That is, from the *p* predictors we choose *k* from them (with $k\\leq p$), and evaluate the training RSS in every possible combination. We keep the one that has the smallest RSS. In contrast, in forward or backward stepwise not every combination is obtained, and thus it is possible that the combination with the smallest RSS is not tested in the training set.\n",
    "\n",
    "b) Regarding the *test* set, the smallest RSS can be obtained from either forward or backward stepwise selection. This is because in the best subset approach, there is a high chance of overfitting, which leads to a poor prediction power in a test set.\n",
    "\n",
    "c) True or False\n",
    "* **i.** **True**. In forward selection, the predictors are added to the set one by one. After choosing *k* predictors, these predictors are kept fixed and then after adding another one we end up with *k+1* predictors. So the *k* previous predictors end up being a subset of the *k+1* predictors at the end.\n",
    "* **ii.** **True**. In backward stepwise, we start with all *p* predictors and remove one by one according to the scores. If we have *k+1* predictors and remove one of them, we end up with *k* predictors, which results in being a subset of the *k+1* previous ones.\n",
    "* **iii.** **False**. When making backward stepwise we may end up with a set of different predictors compared with forward stepwise. Each method has a different starting point (in forward we begin with the null model, while in backward we begin with the full model). Each method may take different selection *paths* and end up with different set of predictors.\n",
    "* **iv.** **False**. This is a similar case than **iii**, but the other way around.\n",
    "* **v.** **False**. Not necessarily true, because choosing *k+1* predictors may no be the same as choosing a set of those *k+1* predictors when choosing only *k*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "a) The lasso, relative to least squares:\n",
    "* **i. Incorrect**. Lasso uses a subset of all the predictors, so it is less flexible than a least squares approach.\n",
    "* **ii. Incorrect**.\n",
    "* **iii. Correct**. Since Lasso tends to reduce the number of predictors, the bias increases while the variance decreases, compared to least squares. If the increase in bias is less than the decrease in variance, the prediction accuracy will improve.\n",
    "* **iv. Incorrect**. Lasso doesn't increase variance nor decrease bias. Is the other way around.\n",
    "\n",
    "b) Ridge regression relative to least squares:\n",
    "* **iii.** The argument is similar to part **a)**.\n",
    "\n",
    "c) Non-linear methods relative to least squares:\n",
    "* **ii.** Non-linear methods are more flexible, and tend to increase bias while reduce variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "a) **iv.** The training RSS will steadily decrease when *s* increases from 0. This is beacuse when *s* increases, the values of $\\beta$ can take larger values, and from the expression of RSS, the RSS will decrease.\n",
    "\n",
    "b) **ii.** For the test RSS, it will first decrease (just as the training RSS), but at some point it will start increasing because of overfitting.\n",
    "\n",
    "c) **iii.*** Variance will steadily increase, because when *s* increases, the parameters become more important.\n",
    "\n",
    "d) **iv.** Squared bias will increase monotonically, since increasing *s* makes the model with more parameters (more biased).\n",
    "\n",
    "e) **v.** The irreducible error does not depend on any parameters, hence it is a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
